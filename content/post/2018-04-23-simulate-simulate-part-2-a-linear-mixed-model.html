---
title: 'Simulate! Simulate! - Part 2: A linear mixed model'
author: Ariel Muldoon
date: '2018-04-23'
slug: simulate-simulate-part-2
categories:
  - r
  - statistics
tags:
  - simulation
  - lmm
  - lme4
draft: FALSE
description: "In my second simulation example I show how to simulate data from a basic two-level hierarchical design.  I go on to explore how well the random effects variance component is estimated for different sample sizes."
---



<p>I feel like I learn something every time start simulating new data to update an assignment or exploring a question from a client via simulation. I‚Äôve seen instances where residual autocorrelation isn‚Äôt detectable when I <em>know</em> it exists (because I simulated it) or I have skewed residuals and/or unequal variances when I simulated residuals from a normal distribution with a single variance. Such results are often due to small sample sizes, which even in this era of big data still isn‚Äôt so unusual in ecology. I‚Äôve found exploring the effect of sample size on statistical results to be quite eye-opening üëÅ.</p>
<p>In <a href="https://aosmith.rbind.io/2018/01/09/simulate-simulate-part1/">my first simulation post</a> I showed how to simulate data for a basic linear model. Today I‚Äôll be talking about how to simulate data for a linear mixed model. So I‚Äôm still working with normally distributed residuals but will add an additional level of variation.</p>
<div id="table-of-contents" class="section level2">
<h2>Table of Contents</h2>
<ul>
<li><a href="#simulate-simulate-dance-to-the-music">Simulate, simulate, dance to the music</a></li>
<li><a href="#the-statistical-model">The statistical model</a></li>
<li><a href="#a-single-simulation-for-the-two-level-model">A single simulation for the two-level model</a></li>
<li><a href="#make-a-function-for-the-simulation">Make a function for the simulation</a></li>
<li><a href="#repeat-the-simulation-many-times">Repeat the simulation many times</a></li>
<li><a href="#extract-results-from-the-linear-mixed-model">Extract results from the linear mixed model</a></li>
<li><a href="#explore-the-effect-of-sample-size-on-variance-estimation">Explore the effect of sample size on variance estimation</a></li>
<li><a href="#an-actual-mixed-model-with-fixed-effects-this-time">An actual mixed model (with fixed effects this time)</a></li>
<li><a href="#just-the-code-please">Just the code, please</a></li>
</ul>
</div>
<div id="simulate-simulate-dance-to-the-music" class="section level1">
<h1>Simulate, simulate, dance to the music</h1>
<p>I learned the basics of linear mixed models in a class where we learned how to analyze data from ‚Äúclassically designed‚Äù experiments. We spent a lot of time writing down the various statistical models in mathematical notation and then fitting said models in SAS. I felt like I understood the basics of mixed models when the class was over (and swore I was done with all the <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> mathematical notation once and for all üòÜ).</p>
<p>It wasn‚Äôt until I started working with clients and teaching labs on mixed models in R that I learned how to do simulations to understand how well such models worked under various scenarios. These simulations took me to a whole new level of understanding of these models and the meaning of all that pesky mathematical notation.</p>
</div>
<div id="the-statistical-model" class="section level1">
<h1>The statistical model</h1>
<p><strong>Danger, equations below!</strong></p>
<p>You can opt to <a href="#a-single-simulation-for-the-two-level-model">jump to the next section</a> if you don‚Äôt find equations useful.</p>
<p>If you don‚Äôt have a good understanding of the statistical model (or even if you do), writing it out in mathematical notation can actually be pretty useful. I‚Äôll write a relatively simple model with random effects below. (Note that I only have an overall mean in this model; I‚Äôll do a short example at the end of the post to show how one can simulate additional fixed effects.)</p>
<p>The study design that is the basis of my model has two different sizes of study units. I‚Äôm using a classic forestry example, where stands of trees are somehow chosen for sampling and then multiple plots within each stand are measured. This is a design with two levels, stands and plots; we could add a third level if individual trees were measured in each plot.</p>
<p>Everything today will be perfectly balanced, so the same number of plots will be sampled in each stand.</p>
<p>I‚Äôm using (somewhat sloppy) ‚Äúregression‚Äù style notation instead of experimental design notation, where the <span class="math inline">\(t\)</span> indexes the observations.</p>
<p><span class="math display">\[y_t = \mu + (b_s)_t + \epsilon_t\]</span></p>
<ul>
<li><span class="math inline">\(y_t\)</span> is the recorded value for the <span class="math inline">\(t\)</span>th observation of the quantitative response variable; <span class="math inline">\(t\)</span> goes from 1 to the number of observations in the dataset. Since plot is the level of observation in this example (i.e., we have a single observation for each plot), <span class="math inline">\(t\)</span> indexes both the number of plots and the number of rows in the dataset.</li>
<li><span class="math inline">\(\mu\)</span> is the overall mean response</li>
<li><span class="math inline">\(b_s\)</span> is the (random) effect of the <span class="math inline">\(s\)</span>th stand on the response. <span class="math inline">\(s\)</span> goes from 1 to the total number of stands sampled. The stand-level random effects are assumed to come from an iid normal distribution with a mean of 0 and some shared, stand-level variance, <span class="math inline">\(\sigma^2_s\)</span>: <span class="math inline">\(b_s \thicksim N(0, \sigma^2_s)\)</span></li>
<li><span class="math inline">\(\epsilon_t\)</span> is the observation-level random effect (the residual error term). Since plots are the level of observation in my scenario, this is essentially the effect of each plot measurement on the response. These are assumed to come from an iid normal distribution with a mean of 0 and some shared variance, <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\epsilon_t \thicksim N(0, \sigma^2)\)</span></li>
</ul>
</div>
<div id="a-single-simulation-for-the-two-level-model" class="section level1">
<h1>A single simulation for the two-level model</h1>
<p>Let‚Äôs jump in and start simulating, as I find the statistical model becomes clearer once we have a simulated dataset to look at.</p>
<p>Here is what the dataset I will create will look like. I have a variable for stands (<code>stand</code>), plots (<code>plot</code>), and the response variable (<code>resp</code>).</p>
<pre><code>#    stand plot      resp
# 1      A    a 10.484415
# 2      A    b  9.946876
# 3      A    c 11.016389
# 4      A    d 11.977799
# 5      B    e 10.322382
# 6      B    f 11.596422
# 7      B    g  9.861173
# 8      B    h  9.003203
# 9      C    i 13.850646
# 10     C    j 12.914153
# 11     C    k 10.529352
# 12     C    l 12.768342
# 13     D    m  7.584302
# 14     D    n  6.568810
# 15     D    o  8.239229
# 16     D    p  5.463744
# 17     E    q 11.981485
# 18     E    r 12.112977
# 19     E    s 13.766137
# 20     E    t 11.429760</code></pre>
<p>I‚Äôll start by setting the seed so these results can be exactly reproduced.</p>
<pre class="r"><code>set.seed(16)</code></pre>
<p>I need to define the ‚Äútruth‚Äù in the simulation by setting all the parameters in the statistical model to a value of my choosing. Here‚Äôs what I‚Äôll do today.</p>
<ul>
<li>The true mean (<span class="math inline">\(\mu\)</span>) will be 10</li>
<li>The stand-level variance (<span class="math inline">\(\sigma^2_s\)</span>) will be set at 4, so the standard deviation (<span class="math inline">\(\sigma_s\)</span>) is 2.</li>
<li>The observation-level random effect variance (<span class="math inline">\(\sigma^2\)</span>) will be set at 1, so the standard deviation (<span class="math inline">\(\sigma\)</span>) is 1.</li>
</ul>
<p>I‚Äôll define the number of groups and number of replicates per group while I‚Äôm at it. I‚Äôll use 5 stands and 4 plots per stand. The total number of plots (and so observations) is the number of stands times the number of plots per stand: <code>5*4 = 20</code>.</p>
<pre class="r"><code>nstand = 5
nplot = 4
mu = 10
sds = 2
sd = 1</code></pre>
<p>I need to create a <code>stand</code> variable, containing unique names for the five sampled stands. I use capital letters for this. Each stand name will be repeated four times, because each one was measured four times (i.e., there are four plots in each stand).</p>
<pre class="r"><code>( stand = rep(LETTERS[1:nstand], each = nplot) )</code></pre>
<pre><code>#  [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;D&quot; &quot;E&quot; &quot;E&quot; &quot;E&quot;
# [20] &quot;E&quot;</code></pre>
<p>I can make a <code>plot</code> variable, as well, although it‚Äôs not needed for modelling since we have a single value per plot. It is fairly common to give plots the same name in each stand (i.e., plots are named 1-4 in each stand), but I‚Äôm a big believer in giving plots unique names. I‚Äôll name plots uniquely using lowercase letters. There are a total of 20 plots.</p>
<pre class="r"><code>( plot = letters[1:(nstand*nplot)] )</code></pre>
<pre><code>#  [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot;
# [20] &quot;t&quot;</code></pre>
<p>Now I will simulate the stand-level random effects. I defined these as <span class="math inline">\(b_s \thicksim N(0, \sigma^2_s)\)</span>, so will randomly draw from a normal distribution with a mean of 0 and standard deviation of 2 (remember that <code>rnorm()</code> in R uses standard deviation, not variance). I have five stands, so I draw five values.</p>
<pre class="r"><code>( standeff = rnorm(nstand, 0, sds) )</code></pre>
<pre><code># [1]  0.9528268 -0.2507600  2.1924324 -2.8884581  2.2956586</code></pre>
<p>Every plot in a stand has the same ‚Äústand effect‚Äù, which I simulated with the five values above. This means that the stand itself is causing the measured response variable to be higher or lower than other stands across all plots in the stand. So the ‚Äústand effect‚Äù must be repeated for every plot in a stand.</p>
<p>The <code>stand</code> variable I made helps me know how to repeat the stand effect values. Based on that variable, every stand effect needs to be repeated four times in a row (once for each plot).</p>
<pre class="r"><code>( standeff = rep(standeff, each = nplot) )</code></pre>
<pre><code>#  [1]  0.9528268  0.9528268  0.9528268  0.9528268 -0.2507600 -0.2507600
#  [7] -0.2507600 -0.2507600  2.1924324  2.1924324  2.1924324  2.1924324
# [13] -2.8884581 -2.8884581 -2.8884581 -2.8884581  2.2956586  2.2956586
# [19]  2.2956586  2.2956586</code></pre>
<p>The observation-level random effect is simulated the same way as for a linear model. Every unique plot measurement has some effect on the response, and that effect is drawn from a normal distribution with a mean of 0 and a standard deviation of 1 (<span class="math inline">\(\epsilon_t \thicksim N(0, \sigma^2)\)</span>).</p>
<p>I make 20 draws from this distribution, one for every plot/observation.</p>
<pre class="r"><code>( ploteff = rnorm(nstand*nplot, 0, sd) )</code></pre>
<pre><code>#  [1] -0.46841204 -1.00595059  0.06356268  1.02497260  0.57314202  1.84718210
#  [7]  0.11193337 -0.74603732  1.65821366  0.72172057 -1.66308050  0.57590953
# [13]  0.47276012 -0.54273166  1.12768707 -1.64779762 -0.31417395 -0.18268157
# [19]  1.47047849 -0.86589878</code></pre>
<p>I‚Äôm going to put all of these variables in a dataset together. This helps me keep things organized for modelling but, more importantly for learning how to do simulations, I think this helps demonstrate how every stand has an overall effect (repeated for every observation in that stand) and every plot has a unique effect. This becomes clear when you peruse the 20-row dataset shown below.</p>
<pre class="r"><code>( dat = data.frame(stand, standeff, plot, ploteff) )</code></pre>
<pre><code>#    stand   standeff plot     ploteff
# 1      A  0.9528268    a -0.46841204
# 2      A  0.9528268    b -1.00595059
# 3      A  0.9528268    c  0.06356268
# 4      A  0.9528268    d  1.02497260
# 5      B -0.2507600    e  0.57314202
# 6      B -0.2507600    f  1.84718210
# 7      B -0.2507600    g  0.11193337
# 8      B -0.2507600    h -0.74603732
# 9      C  2.1924324    i  1.65821366
# 10     C  2.1924324    j  0.72172057
# 11     C  2.1924324    k -1.66308050
# 12     C  2.1924324    l  0.57590953
# 13     D -2.8884581    m  0.47276012
# 14     D -2.8884581    n -0.54273166
# 15     D -2.8884581    o  1.12768707
# 16     D -2.8884581    p -1.64779762
# 17     E  2.2956586    q -0.31417395
# 18     E  2.2956586    r -0.18268157
# 19     E  2.2956586    s  1.47047849
# 20     E  2.2956586    t -0.86589878</code></pre>
<p>I now have the fixed values of the parameters, the variable <code>stand</code> to represent the random effect in a model, and the simulated effects of stands and plots drawn from their defined distributions. That‚Äôs all the pieces I need to calculate my response variable.</p>
<p>The statistical model</p>
<p><span class="math display">\[y_t = \mu + (b_s)_t + \epsilon_t\]</span></p>
<p>is my guide for how to combine these pieces to create the simulated response variable, <span class="math inline">\(y_t\)</span>. Notice I call the simulated response variable <code>resp</code>.</p>
<pre class="r"><code>( dat$resp = with(dat, mu + standeff + ploteff ) )</code></pre>
<pre><code>#  [1] 10.484415  9.946876 11.016389 11.977799 10.322382 11.596422  9.861173
#  [8]  9.003203 13.850646 12.914153 10.529352 12.768342  7.584302  6.568810
# [15]  8.239229  5.463744 11.981485 12.112977 13.766137 11.429760</code></pre>
<p>It‚Äôs time for model fitting! I can fit a model with two sources of variation (stand and plot) with, e.g., the <code>lmer()</code> function from package <strong>lme4</strong>.</p>
<pre class="r"><code>library(lme4) # v. 1.1-21</code></pre>
<p>The results for the estimated overall mean and standard deviations of random effects in this model look pretty similar to my defined parameter values.</p>
<pre class="r"><code>fit1 = lmer(resp ~ 1 + (1|stand), data = dat)
fit1</code></pre>
<pre><code># Linear mixed model fit by REML [&#39;lmerMod&#39;]
# Formula: resp ~ 1 + (1 | stand)
#    Data: dat
# REML criterion at convergence: 72.5943
# Random effects:
#  Groups   Name        Std.Dev.
#  stand    (Intercept) 2.168   
#  Residual             1.130   
# Number of obs: 20, groups:  stand, 5
# Fixed Effects:
# (Intercept)  
#       10.57</code></pre>
</div>
<div id="make-a-function-for-the-simulation" class="section level1">
<h1>Make a function for the simulation</h1>
<p>A single simulation can help us understand the statistical model, but usually the goal of a simulation is to see how the model behaves over the long run. To repeat this simulation many times in R we‚Äôll want to ‚Äúfunctionize‚Äù the data simulating and model fitting process.</p>
<p>In my function I‚Äôm going to set all the arguments to the parameter values as I defined them above. I allow some flexibility, though, so the argument values can be changed if I want to explore the simulation with, say, a different number of replications or different standard deviations at either level.</p>
<p>This function returns a linear model fit with <code>lmer()</code>.</p>
<pre class="r"><code>twolevel_fun = function(nstand = 5, nplot = 4, mu = 10, sigma_s = 2, sigma = 1) {
     standeff = rep( rnorm(nstand, 0, sigma_s), each = nplot)
     stand = rep(LETTERS[1:nstand], each = nplot)
     ploteff = rnorm(nstand*nplot, 0, sigma)
     resp = mu + standeff + ploteff
     dat = data.frame(stand, resp)
     lmer(resp ~ 1 + (1|stand), data = dat)
}</code></pre>
<p>I test the function, using the same <code>seed</code>, to make sure things are working as expected and that I get the same results as above.</p>
<pre class="r"><code>set.seed(16)
twolevel_fun()</code></pre>
<pre><code># Linear mixed model fit by REML [&#39;lmerMod&#39;]
# Formula: resp ~ 1 + (1 | stand)
#    Data: dat
# REML criterion at convergence: 72.5943
# Random effects:
#  Groups   Name        Std.Dev.
#  stand    (Intercept) 2.168   
#  Residual             1.130   
# Number of obs: 20, groups:  stand, 5
# Fixed Effects:
# (Intercept)  
#       10.57</code></pre>
</div>
<div id="repeat-the-simulation-many-times" class="section level1">
<h1>Repeat the simulation many times</h1>
<p>Now that I have a working function to simulate data and fit the model it‚Äôs time to do the simulation many times. The model from each individual simulation is saved to allow exploration of long run model performance.</p>
<p>This is a task for <code>replicate()</code>, which repeatedly calls a function and saves the output. When using <code>simplify = FALSE</code> the output is a list, which is convenient for going through to extract elements from the models later. I‚Äôll re-run the simulation 100 times as an example, although I will do 1000 runs later when I explore the long-run performance of variance estimates.</p>
<pre class="r"><code>sims = replicate(100, twolevel_fun(), simplify = FALSE )
sims[[100]]</code></pre>
<pre><code># Linear mixed model fit by REML [&#39;lmerMod&#39;]
# Formula: resp ~ 1 + (1 | stand)
#    Data: dat
# REML criterion at convergence: 58.0201
# Random effects:
#  Groups   Name        Std.Dev.
#  stand    (Intercept) 1.7197  
#  Residual             0.7418  
# Number of obs: 20, groups:  stand, 5
# Fixed Effects:
# (Intercept)  
#       7.711</code></pre>
</div>
<div id="extract-results-from-the-linear-mixed-model" class="section level1">
<h1>Extract results from the linear mixed model</h1>
<p>After running all the models we will want to extract whatever we are interested in. The <code>tidy()</code> function from package <strong>broom</strong> can be used to conveniently extract both fixed and random effects.</p>
<p>Below is an example on the practice model. You‚Äôll notice there are no p-values for fixed effects. If those are desired and the degrees of freedom can be calculated, see packages <strong>lmerTest</strong> and <a href="https://github.com/bbolker/broom.mixed"><strong>broom.mixed</strong></a>.</p>
<pre class="r"><code>library(broom) # v. 0.5.6
tidy(fit1)</code></pre>
<pre><code># # A tibble: 3 x 5
#   term                    estimate std.error statistic group   
#   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   
# 1 (Intercept)                10.6       1.00      10.5 fixed   
# 2 sd_(Intercept).stand        2.17     NA         NA   stand   
# 3 sd_Observation.Residual     1.13     NA         NA   Residual</code></pre>
<p>If we want to extract only the fixed effects:</p>
<pre class="r"><code>tidy(fit1, effects = &quot;fixed&quot;)</code></pre>
<pre><code># # A tibble: 1 x 4
#   term        estimate std.error statistic
#   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
# 1 (Intercept)     10.6      1.00      10.5</code></pre>
<p>And for the random effects, which can be pulled out as variances via <code>scales</code> instead of the default standard deviations:</p>
<pre class="r"><code>tidy(fit1, effects = &quot;ran_pars&quot;, scales = &quot;vcov&quot;)</code></pre>
<pre><code># # A tibble: 2 x 3
#   term                     group    estimate
#   &lt;chr&gt;                    &lt;chr&gt;       &lt;dbl&gt;
# 1 var_(Intercept).stand    stand        4.70
# 2 var_Observation.Residual Residual     1.28</code></pre>
</div>
<div id="explore-the-effect-of-sample-size-on-variance-estimation" class="section level1">
<h1>Explore the effect of sample size on variance estimation</h1>
<p>Today I‚Äôll look at how well we estimate variances of random effects for different samples sizes. I‚Äôll simulate data for sampling 5 stands, 20 stands, and 100 stands.</p>
<p>I‚Äôm going to load some helper packages for this, including <strong>purrr</strong> for looping, <strong>dplyr</strong> for data manipulation tasks, and <strong>ggplot2</strong> for plotting.</p>
<pre class="r"><code>library(purrr) # v. 0.3.3
suppressPackageStartupMessages( library(dplyr) ) # v. 0.8.3
library(ggplot2) # v. 3.2.1</code></pre>
<p>I‚Äôm going to loop through a vector of the three stand sample sizes and then simulate data and fit a model 1000 times for each one. I‚Äôm using <strong>purrr</strong> functions for this, and I end up with a list of lists (1000 models for each sample size). It takes a minute or two to fit the 3000 models. I‚Äôm ignoring all warning messages (but this isn‚Äôt always a good idea).</p>
<pre class="r"><code>stand_sims = c(5, 20, 100) %&gt;%
     set_names() %&gt;%
     map(~replicate(1000, twolevel_fun(nstand = .x) ) )</code></pre>
<p>Next I‚Äôll pull out the stand variance for each model via <code>tidy()</code>.</p>
<p>I use <code>modify_depth()</code> to work on the nested (innermost) list, and then row bind the nested lists into a data.frame to get things in a convenient format for plotting. I finish by filtering things to keep only the <code>stand</code> variance, as I extracted both stand and residual variances from the model.</p>
<pre class="r"><code>stand_vars = stand_sims %&gt;%
     modify_depth(2, ~tidy(.x, effects = &quot;ran_pars&quot;, scales = &quot;vcov&quot;) ) %&gt;%
     map_dfr(bind_rows, .id = &quot;stand_num&quot;) %&gt;%
     filter(group == &quot;stand&quot;)
head(stand_vars)</code></pre>
<pre><code># # A tibble: 6 x 4
#   stand_num term                  group estimate
#   &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;    &lt;dbl&gt;
# 1 5         var_(Intercept).stand stand     2.53
# 2 5         var_(Intercept).stand stand     7.70
# 3 5         var_(Intercept).stand stand     4.19
# 4 5         var_(Intercept).stand stand     3.28
# 5 5         var_(Intercept).stand stand    12.9 
# 6 5         var_(Intercept).stand stand     4.62</code></pre>
<p>Let‚Äôs take a look at the distributions of the variances for each sample size via density plots. We know the true variance is 4, so I‚Äôll add a vertical line at 4.</p>
<pre class="r"><code>ggplot(stand_vars, aes(x = estimate) ) +
     geom_density(fill = &quot;blue&quot;, alpha = .25) +
     facet_wrap(~stand_num) +
     geom_vline(xintercept = 4)</code></pre>
<p><img src="/post/2018-04-23-simulate-simulate-part-2-a-linear-mixed-model_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Whoops, I need to get my factor levels in order.</p>
<pre class="r"><code>stand_vars = mutate(stand_vars, stand_num = forcats::fct_inorder(stand_num) )</code></pre>
<p>I‚Äôll also add some clearer labels for the facets.</p>
<pre class="r"><code>add_prefix = function(string) {
     paste(&quot;Number stands:&quot;, string, sep = &quot; &quot;)
}</code></pre>
<p>And finally I‚Äôll add the median of each distribution as a second vertical line.</p>
<pre class="r"><code>groupmed = stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise(mvar = median(estimate) )</code></pre>
<p>Looking at the plots we can really see how poorly we can estimate variances when we have few replications. When only 5 stands are sampled, the variance can be estimated as low as 0 and as high as ~18 (üòÆ) when it‚Äôs really 4.</p>
<p>By the time we have 20 stands things look better, and things look quite good with 100 stands (although notice variance still ranges from 1 to ~8).</p>
<pre class="r"><code>ggplot(stand_vars, aes(x = estimate) ) + 
     geom_density(fill = &quot;blue&quot;, alpha = .25) +
     facet_wrap(~stand_num, labeller = as_labeller(add_prefix) ) +
     geom_vline(aes(xintercept = 4, linetype = &quot;True variance&quot;), size = .5 ) +
     geom_vline(data = groupmed, aes(xintercept = mvar, linetype = &quot;Median variance&quot;),
                size = .5) +
     theme_bw(base_size = 14) +
     scale_linetype_manual(name = &quot;&quot;, values = c(2, 1) ) +
     theme(legend.position = &quot;bottom&quot;,
           legend.key.width = unit(.1, &quot;cm&quot;) ) +
     labs(x = &quot;Estimated Variance&quot;, y = NULL)</code></pre>
<p><img src="/post/2018-04-23-simulate-simulate-part-2-a-linear-mixed-model_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Here are some additional descriptive statistics of the distribution of variances in each group to complement the info in the plot.</p>
<pre class="r"><code>stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise_at(&quot;estimate&quot;, 
                  list(min = min, mean = mean, med = median, max = max) )</code></pre>
<pre><code># # A tibble: 3 x 5
#   stand_num   min  mean   med   max
#   &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
# 1 5         0      4.04  3.39 17.6 
# 2 20        0.994  4.03  3.88  9.15
# 3 100       1.27   4.04  3.92  8.71</code></pre>
<p>So how much of the distribution is below 4 for each estimate?</p>
<p>We can see for 5 samples that the variance is definitely underestimated more often than it is overestimated; almost 60% of the distribution is below the true variance of 4.</p>
<p>Every time I do this sort of simulation I am newly surprised that even large samples tend to underestimate variances slightly more often than overestimate them.</p>
<pre class="r"><code>stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise(mean(estimate &lt; 4) )</code></pre>
<pre><code># # A tibble: 3 x 2
#   stand_num `mean(estimate &lt; 4)`
#   &lt;fct&gt;                    &lt;dbl&gt;
# 1 5                        0.577
# 2 20                       0.532
# 3 100                      0.525</code></pre>
<p>My take home from all of this? We may need to be cautious with results for studies where the goal is to make inference about the estimates of the variances or for testing variables measured at the level of the largest unit when the number of units is small.</p>
</div>
<div id="an-actual-mixed-model-with-fixed-effects-this-time" class="section level1">
<h1>An actual mixed model (with fixed effects this time)</h1>
<p>I simplified things above so much that I didn‚Äôt have any fixed effects variables. We can certainly include fixed effects in a simulation.</p>
<p>Below is a quick example. I‚Äôll create two continuous variables, one measured at the stand level and one measured at the plot level, that have linear relationships with the mean response variable. The study is the same as the one I defined for the previous simulation.</p>
<p>Here‚Äôs the new statistical model.</p>
<p><span class="math display">\[y_t = \beta_0 + \beta_1*(Elevation_s)_t + \beta_2*Slope_t + (b_s)_t + \epsilon_t\]</span></p>
<p>Where</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the mean response when both Elevation and Slope are 0<br />
</li>
<li><span class="math inline">\(\beta_1\)</span> is the change in mean response for a 1-unit change in elevation. Elevation is measured at the stand level, so all plots in a stand share a single value of elevation.<br />
</li>
<li><span class="math inline">\(\beta_2\)</span> is the change in mean response for a 1-unit change in slope. Slope is measured at the plot level, so every plot potentially has a unique value of slope.</li>
</ul>
<p>Setting the values for the three new parameters and simulating values for the continuous explanatory variables will be additional steps in the simulation. The random effects are simulated the same way as before.</p>
<p>I define the new parameters below.</p>
<ul>
<li>The intercept (<span class="math inline">\(\beta_0\)</span>) will be -1</li>
<li>The coefficient for elevation (<span class="math inline">\(\beta_1\)</span>) will be set to 0.005</li>
<li>The coefficient for slope (<span class="math inline">\(\beta_2\)</span>) will be set to 0.1</li>
</ul>
<pre class="r"><code>nstand = 5
nplot = 4
b0 = -1
b1 = .005
b2 = .1
sds = 2
sd = 1</code></pre>
<p>Here are the variables I simulated previously.</p>
<pre class="r"><code>set.seed(16)
stand = rep(LETTERS[1:nstand], each = nplot)
standeff = rep( rnorm(nstand, 0, sds), each = nplot)
ploteff = rnorm(nstand*nplot, 0, sd)</code></pre>
<p>I will simulate the explanatory variables by randomly drawing from uniform distributions via <code>runif()</code>. I change the minimum and maximum values of the uniform distribution as needed to get an appropriate spread for a given variable. If the distribution of your explanatory variables are more skewed you could use a different distribution (like the Gamma distribution).</p>
<p>First I simulate values for elevation. This variable only five values, as it is a stand-level variable. I need to repeat each value for the four plots measured in each stand like I did when making the <code>stand</code> variable.</p>
<pre class="r"><code>( elevation = rep( runif(nstand, 1000, 1500), each = nplot) )</code></pre>
<pre><code>#  [1] 1468.339 1468.339 1468.339 1468.339 1271.581 1271.581 1271.581 1271.581
#  [9] 1427.050 1427.050 1427.050 1427.050 1166.014 1166.014 1166.014 1166.014
# [17] 1424.256 1424.256 1424.256 1424.256</code></pre>
<p>I can simulate slope the same way, pulling random values from a uniform distribution with different limits. The slope is measured at the plot level, so I have one value for every plot in the dataset.</p>
<pre class="r"><code>( slope = runif(nstand*nplot, 2, 75) )</code></pre>
<pre><code>#  [1] 48.45477 60.37014 59.58588 44.76939 61.88313 20.29559 71.51617 42.35035
#  [9] 63.67044 36.43613 10.58778 14.62304 35.11192 13.19697  9.29946 46.56462
# [17] 34.68245 53.61456 37.00606 42.30044</code></pre>
<p>We now have everything we need to create the response variable.</p>
<p>Based on our equation <span class="math display">\[y_t = \beta_0 + \beta_1*(Elevation_s)_t + \beta_2*Slope_t + (b_s)_t + \epsilon_t\]</span></p>
<p>the response variable will be calculated via</p>
<pre class="r"><code>( resp2 = b0 + b1*elevation + b2*slope + standeff + ploteff )</code></pre>
<pre><code>#  [1] 11.671585 12.325584 13.316671 12.796432 11.868602  8.983889 12.370697
#  [8]  8.596145 16.352939 12.693014  7.723378 10.365894  5.925563  2.718576
# [15]  3.999244  4.950275 11.571009 13.595712 13.588022 11.781083</code></pre>
<p>Now we can fit a mixed model for <code>resp2</code> with <code>elevation</code> and <code>slope</code> as fixed effects, <code>stand</code> as the random effect and the residual error term based on plot-to-plot variation. (<em>Notice I didn‚Äôt put these variables in a dataset, which I usually like to do to keep things organized and to avoid problems of vectors in my environment getting overwritten by mistake.</em>)</p>
<p>We can see some of the estimates in this one model aren‚Äôt very similar to our set values, and doing a full simulation would allow us to explore the variation in the estimates. For example, I expect the coefficient for elevation, based on only five values, will be extremely unstable.</p>
<pre class="r"><code>lmer(resp2 ~ elevation + slope + (1|stand) )</code></pre>
<pre><code># Linear mixed model fit by REML [&#39;lmerMod&#39;]
# Formula: resp2 ~ elevation + slope + (1 | stand)
# REML criterion at convergence: 81.9874
# Random effects:
#  Groups   Name        Std.Dev.
#  stand    (Intercept) 1.099   
#  Residual             1.165   
# Number of obs: 20, groups:  stand, 5
# Fixed Effects:
# (Intercept)    elevation        slope  
#   -21.31463      0.02060      0.09511</code></pre>
</div>
<div id="just-the-code-please" class="section level1">
<h1>Just the code, please</h1>
<p>Here‚Äôs the code without all the discussion. Copy and paste the code below or you can download an R script of uncommented code <a href="/script/2018-04-23-simulate-simulate-part-2-a-linear-mixed-model.R">from here</a>.</p>
<pre class="r"><code>set.seed(16)
nstand = 5
nplot = 4
mu = 10
sds = 2
sd = 1

( stand = rep(LETTERS[1:nstand], each = nplot) )
( plot = letters[1:(nstand*nplot)] )
( standeff = rnorm(nstand, 0, sds) )
( standeff = rep(standeff, each = nplot) )
( ploteff = rnorm(nstand*nplot, 0, sd) )

( dat = data.frame(stand, standeff, plot, ploteff) )

( dat$resp = with(dat, mu + standeff + ploteff ) )

library(lme4) # v. 1.1-21

fit1 = lmer(resp ~ 1 + (1|stand), data = dat)
fit1

twolevel_fun = function(nstand = 5, nplot = 4, mu = 10, sigma_s = 2, sigma = 1) {
     standeff = rep( rnorm(nstand, 0, sigma_s), each = nplot)
     stand = rep(LETTERS[1:nstand], each = nplot)
     ploteff = rnorm(nstand*nplot, 0, sigma)
     resp = mu + standeff + ploteff
     dat = data.frame(stand, resp)
     lmer(resp ~ 1 + (1|stand), data = dat)
}

set.seed(16)
twolevel_fun()

sims = replicate(100, twolevel_fun(), simplify = FALSE )
sims[[100]]

library(broom) # v. 0.5.6
tidy(fit1)

tidy(fit1, effects = &quot;fixed&quot;)

tidy(fit1, effects = &quot;ran_pars&quot;, scales = &quot;vcov&quot;)

library(purrr) # v. 0.3.3
suppressPackageStartupMessages( library(dplyr) ) # v. 0.8.3
library(ggplot2) # v. 3.2.1

stand_sims = c(5, 20, 100) %&gt;%
     set_names() %&gt;%
     map(~replicate(1000, twolevel_fun(nstand = .x) ) )

stand_vars = stand_sims %&gt;%
     modify_depth(2, ~tidy(.x, effects = &quot;ran_pars&quot;, scales = &quot;vcov&quot;) ) %&gt;%
     map_dfr(bind_rows, .id = &quot;stand_num&quot;) %&gt;%
     filter(group == &quot;stand&quot;)
head(stand_vars)

ggplot(stand_vars, aes(x = estimate) ) +
     geom_density(fill = &quot;blue&quot;, alpha = .25) +
     facet_wrap(~stand_num) +
     geom_vline(xintercept = 4)

stand_vars = mutate(stand_vars, stand_num = forcats::fct_inorder(stand_num) )

add_prefix = function(string) {
     paste(&quot;Number stands:&quot;, string, sep = &quot; &quot;)
}

groupmed = stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise(mvar = median(estimate) )

ggplot(stand_vars, aes(x = estimate) ) + 
     geom_density(fill = &quot;blue&quot;, alpha = .25) +
     facet_wrap(~stand_num, labeller = as_labeller(add_prefix) ) +
     geom_vline(aes(xintercept = 4, linetype = &quot;True variance&quot;), size = .5 ) +
     geom_vline(data = groupmed, aes(xintercept = mvar, linetype = &quot;Median variance&quot;),
                size = .5) +
     theme_bw(base_size = 14) +
     scale_linetype_manual(name = &quot;&quot;, values = c(2, 1) ) +
     theme(legend.position = &quot;bottom&quot;,
           legend.key.width = unit(.1, &quot;cm&quot;) ) +
     labs(x = &quot;Estimated Variance&quot;, y = NULL)

stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise_at(&quot;estimate&quot;, 
                  list(min = min, mean = mean, med = median, max = max) )

stand_vars %&gt;%
     group_by(stand_num) %&gt;%
     summarise(mean(estimate &lt; 4) )

nstand = 5
nplot = 4
b0 = -1
b1 = .005
b2 = .1
sds = 2
sd = 1

set.seed(16)
stand = rep(LETTERS[1:nstand], each = nplot)
standeff = rep( rnorm(nstand, 0, sds), each = nplot)
ploteff = rnorm(nstand*nplot, 0, sd)

( elevation = rep( runif(nstand, 1000, 1500), each = nplot) )
( slope = runif(nstand*nplot, 2, 75) )
( resp2 = b0 + b1*elevation + b2*slope + standeff + ploteff )

lmer(resp2 ~ elevation + slope + (1|stand) )</code></pre>
</div>
